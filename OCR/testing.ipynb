{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_document(document):\n",
    "    # Tokenize the document\n",
    "    tokens = word_tokenize(document.lower())  # Convert to lowercase for consistency\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a preprocessed document\n",
    "    preprocessed_document = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess documents\n",
    "preprocessed_documents = [preprocess_document(doc) for doc in raw_documents]\n",
    "\n",
    "# Train a Doc2Vec model\n",
    "documents = [TaggedDocument(words=word_tokenize(doc), tags=[str(i)]) for i, doc in enumerate(preprocessed_documents)]\n",
    "model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.build_vocab(documents)\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "# Get embeddings for preprocessed documents\n",
    "embeddings = [model.dv[i] for i in range(len(preprocessed_documents))]\n",
    "\n",
    "# Standardize the embeddings\n",
    "scaler = StandardScaler()\n",
    "embeddings_standardized = scaler.fit_transform(embeddings)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2)\n",
    "labels = dbscan.fit_predict(embeddings_standardized)\n",
    "\n",
    "# Analyze clusters and anomalies\n",
    "unique_labels = set(labels)\n",
    "for i, label in enumerate(labels):\n",
    "    if label == -1:\n",
    "        print(f'Document {i} is an anomaly.')\n",
    "    else:\n",
    "        print(f'Document {i} belongs to cluster {label}.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
