{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOADING THE DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers python-docx PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from transformers import pipeline\n",
    "from docx import Document\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pytesseract\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTENCE TRANSFORMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-mpnet-base-v2\"\n",
    "sentence_encoder = SentenceTransformer(model_name)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "  print(\"\\nPreprocessing the text . . . \")\n",
    "  \"\"\"\n",
    "  This function preprocesses text data for anomaly detection.\n",
    "\n",
    "  Args:\n",
    "      text: The text string to be preprocessed.\n",
    "\n",
    "  Returns:\n",
    "      A list of preprocessed tokens (words).\n",
    "  \"\"\"\n",
    "  # Lowercase the text\n",
    "  text = text.lower()\n",
    "\n",
    "  # Tokenize the text (split into words)\n",
    "  words = text.split()\n",
    "\n",
    "  # Remove punctuation (optional)\n",
    "  punctuation = [\".\", \",\", \"!\", \"?\", \";\", \":\"]\n",
    "  words = [word for word in words if word not in punctuation]\n",
    "\n",
    "  # Remove stop words (common words)\n",
    "  stop_words = stopwords.words('english')\n",
    "  words = [word for word in words if word not in stop_words]\n",
    "\n",
    "  # Lemmatization (optional, reduces words to their base form)\n",
    "  # lemmatizer = WordNetLemmatizer()\n",
    "  # words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "  return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODING THE SENTENCES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_statement(statement):\n",
    "  print(\"\\nEncoding each statement . . . \")\n",
    "  \"\"\"\n",
    "  This function preprocesses a statement and generates its sentence embedding.\n",
    "\n",
    "  Args:\n",
    "      statement: The text of the statement.\n",
    "\n",
    "  Returns:\n",
    "      A sentence embedding vector.\n",
    "  \"\"\"\n",
    "  # Preprocess the statement\n",
    "  preprocessed_text = preprocess_text(statement)\n",
    "\n",
    "  # Encode the preprocessed statement\n",
    "  model_name = \"all-mpnet-base-v2\"  # Replace with your chosen model\n",
    "  model = SentenceTransformer(model_name)\n",
    "  statement_vector = model.encode(preprocessed_text)\n",
    "  return statement_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETECTING TOPIC KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_topics_keywords(statements, num_topics):\n",
    "  print(\"\\nFinding out the topics using the LDA { Latent Dirichlet Algorithm } among the provided set of content . . . \")\n",
    "  \"\"\"\n",
    "  This function detects topics and keywords from a corpus of statements using LDA.\n",
    "\n",
    "  Args:\n",
    "      statements: A list of statement text strings.\n",
    "      num_topics: The number of topics to identify.\n",
    "\n",
    "  Returns:\n",
    "      A list of topic keywords, where each element is a list of keywords\n",
    "      representing a topic.\n",
    "  \"\"\"\n",
    "  # Preprocess statements (replace with your preferred preprocessing)\n",
    "  preprocessed_statements = [preprocess_text(statement) for statement in statements]\n",
    "  print(\"\\nThe statements were successfully preprocessed\")\n",
    "  # Create a dictionary from preprocessed statements\n",
    "  dictionary = Dictionary(preprocessed_statements)\n",
    "  print(\"\\nThe preprocessed text was succesffully converted into a dictionary\")\n",
    "\n",
    "  # Convert statements to bag-of-words format\n",
    "  corpus = [dictionary.doc2bow(statement) for statement in preprocessed_statements]\n",
    "\n",
    "  # Train the LDA model\n",
    "  print(\"\\nThe LDA Model is running to detect the topics\")\n",
    "  lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "  # Extract topic keywords\n",
    "  topic_keywords = []\n",
    "  for topic_id in range(lda_model.num_topics):\n",
    "    topic_words = [word for word, prob in lda_model.show_topic(topic_id, topn=10)]  # Top 10 keywords per topic\n",
    "    topic_keywords.append(topic_words)\n",
    "\n",
    "  return topic_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETTING TOPIC KEYWORDS\n",
    "\n",
    "From a specific statement getting the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(statement):\n",
    "  \"\"\"\n",
    "  This function extracts keywords from a statement using NLTK (replace with your preferred method).\n",
    "\n",
    "  Args:\n",
    "      statement: The text of the statement.\n",
    "\n",
    "  Returns:\n",
    "      A list of extracted keywords (lowercase).\n",
    "  \"\"\"\n",
    "  print(\"\\nFinding out the keywords . . . \")\n",
    "  words = nltk.word_tokenize(statement.lower())\n",
    "\n",
    "  # need to be adjusted\n",
    "  # Filter for nouns, verbs, and named entities (adjust based on your needs)\n",
    "  keywords = [word for word in words if (\n",
    "      nltk.pos_tag([word])[0][1] in [\"NN\", \"NNP\", \"VB\", \"VBP\"]\n",
    "  )]\n",
    "  return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN ANOMALY DETECTION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_hybrid(statements, your_keywords):\n",
    "  print(\"\\nDetecting the anomalies on the statements using the keywords . . . \")\n",
    "  \"\"\"\n",
    "  This function detects anomalies in a list of statements using cosine similarity\n",
    "  and keyword heuristics.\n",
    "\n",
    "  Args:\n",
    "      statements: A list of statement text strings.\n",
    "      your_keywords: A list of keywords relevant to the topic.\n",
    "\n",
    "  Returns:\n",
    "      A list of tuples, where each tuple contains the indices of two statements\n",
    "      with potential contradictions.\n",
    "  \"\"\"\n",
    "  # Encode each statement\n",
    "  statement_vectors = [encode_statement(statement) for statement in statements]\n",
    "\n",
    "  # Set minimum number of shared keywords for anomaly consideration\n",
    "  min_keywords = 2  # Adjust threshold based on your data\n",
    "\n",
    "  anomaly_pairs = []\n",
    "  for i in range(len(statement_vectors)):\n",
    "    for j in range(i + 1, len(statement_vectors)):\n",
    "      # Check if statements share at least the minimum keywords\n",
    "      shared_keywords = len(set(get_keywords(statements[i])) & set(get_keywords(statements[j])))\n",
    "      if shared_keywords >= min_keywords:\n",
    "        # Calculate cosine similarity\n",
    "        element_wise_product = np.outer(statement_vectors[i], statement_vectors[j])\n",
    "        norm_product = np.linalg.norm(statement_vectors[i]) * np.linalg.norm(statement_vectors[j])\n",
    "        similarity = np.sum(element_wise_product) / (norm_product + 1e-8)\n",
    "\n",
    "        # Identify potential anomalies based on threshold\n",
    "        threshold = 0.7  # Adjust threshold based on your data and desired strictness\n",
    "        if similarity < threshold:\n",
    "          anomaly_pairs.append((i, j))\n",
    "  print(\"The Anomaly Detection is succesfully completed!\\n\")\n",
    "  print(\"************************************************\\n\")\n",
    "  return anomaly_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTING STATEMENTS FROM A DOCUMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing the statements of the page extracted content for escape sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_page_content(page_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes escape sequences from a given string containing PDF content.\n",
    "\n",
    "    Args:\n",
    "        page_content (str): The text content extracted from a PDF page.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text with escape sequences removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub(r'\\\\(.)', r'\\1', page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACTING THE STATEMENTS PAGE WISE AND STORING THE CONTENT IN RESPECTIVE PAGE INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pdf(pdf_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Preprocesses the content of a PDF by removing escape sequences and other unwanted\n",
    "    characters from each page. Stores the preprocessed text in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing page numbers as keys and preprocessed text as values.\n",
    "    \"\"\"\n",
    "\n",
    "    preprocessed_data = {}\n",
    "    try:\n",
    "        # Open the PDF file in binary mode\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PdfReader(pdf_file)\n",
    "\n",
    "            # Process each page\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                page_text = page.extract_text().strip()\n",
    "\n",
    "                # Preprocess text\n",
    "                preprocessed_text = preprocess_page_content(page_text)\n",
    "\n",
    "                # Store preprocessed text in the dictionary\n",
    "                preprocessed_data[page_num + 1] = preprocessed_text\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: PDF file '{pdf_file.name}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the PDF: {e}\")\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# pdf_path = \"60009210051_CL_Assignment1.pdf\"  # Replace with your actual PDF path\n",
    "# preprocessed_pdf_data = preprocess_pdf(pdf_path)\n",
    "\n",
    "# # Print the JSON string if preprocessed data is available\n",
    "# if preprocessed_pdf_data:\n",
    "#     json_string = json.dumps(preprocessed_pdf_data, indent=4)\n",
    "#     print(json_string)\n",
    "# else:\n",
    "#     print(\"No content found in the PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOISE REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_removal(image):\n",
    "    kernel = np.ones((1,1), np.uint8)\n",
    "    image = cv2.dilate(image , kernel, iterations=1)\n",
    "    kernel = np.ones((1,1), np.uint8)\n",
    "    image = cv2.erode(image,kernel,iterations=1)\n",
    "    image = cv2.morphologyEx(image , cv2.MORPH_CLOSE, kernel)\n",
    "    image = cv2.medianBlur(image , 3)\n",
    "    return (image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVING BORDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_borders(image):\n",
    "    contours, heirarchy = cv2.findContours(image , cv2.RETR_EXTERNAL , cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cntSorted = sorted(contours, key=lambda x:cv2.contourArea(x))\n",
    "    cnt = cntSorted[-1]\n",
    "\n",
    "    x,y,w,h = cv2.boundingRect(cnt)\n",
    "    crop = image[y:y+h,x:x+w]\n",
    "    return crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THINNING THE FONT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thin_font(image):\n",
    "    image = cv2.bitwise_not(image)\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    image = cv2.erode(image , kernel, iterations=1)\n",
    "    image = cv2.bitwise_not(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THICKENING THE BORDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thick_font(image):\n",
    "    image = cv2.bitwise_not(image)\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    image = cv2.dilate(image , kernel, iterations=1)\n",
    "    image = cv2.bitwise_not(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLORS FOR THE BOUNDING BOXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = [255,255,255]\n",
    "top , bottom , left , right = [150]*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING THE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(im_file):\n",
    "    img = cv2.imread(im_file)\n",
    "    gray_image= grayscale(img)\n",
    "    thresh , im_bw = cv2.threshold(gray_image , 210 , 230, cv2.THRESH_BINARY)\n",
    "    no_noise = noise_removal(im_bw)\n",
    "    # thin = thin_font(no_noise)\n",
    "    # no_borders = remove_borders(no_noise)\n",
    "    # image_with_border = cv2.copyMakeBorder(no_borders, top, bottom , left , right, cv2.BORDER_CONSTANT , value=color)\n",
    "    return no_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISPLAYING THE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying-different-images-with-actual-size-in-matplotlib-subplot\n",
    "def display(im_path):\n",
    "    dpi = 80\n",
    "    im_data = plt.imread(im_path)\n",
    "\n",
    "    height, width  = im_data.shape[:2]\n",
    "    \n",
    "    # What size does the figure need to be in inches to fit the image?\n",
    "    figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "    # Create a figure of the right size with one axes that takes up the full figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "    # Hide spines, ticks, etc.\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Display the image.\n",
    "    ax.imshow(im_data, cmap='gray')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORMING THE MAIN OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_image(image_path):\n",
    "    final_image = preprocess_image(image_path)\n",
    "    cv2.imwrite(\"output/final_test.jpg\", final_image)\n",
    "    extracted_text = pytesseract.image_to_string(\"output/final_test.jpg\")\n",
    "    return final_image ,extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEARCHING FOR A SPECIFIC LINE IN A GIVEN DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_line_in_json(preprocessed_data: dict, target_line: str) -> list[int]:\n",
    "    \"\"\"\n",
    "    Searches for a specific line within the preprocessed JSON data\n",
    "    and returns a list of page numbers where the line is found.\n",
    "\n",
    "    Args:\n",
    "        preprocessed_data (dict): A dictionary containing page numbers as keys\n",
    "                                  and preprocessed text as values.\n",
    "        target_line (str): The line to search for.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of page numbers where the target line is found.\n",
    "    \"\"\"\n",
    "\n",
    "    found_pages = []\n",
    "    for page_num, page_content in preprocessed_data.items():\n",
    "        if target_line in page_content:\n",
    "            found_pages.append(page_num)\n",
    "\n",
    "    return found_pages\n",
    "\n",
    "\n",
    "#  Example usage:\n",
    "# pdf_path = \"60009210051_CL_Assignment1.pdf\"  # Replace with your actual PDF path\n",
    "# target_line = \"Given a corpus C2, the Maximum Likelihood Estimation (MLE) for the bigram\"\n",
    "\n",
    "# preprocessed_pdf_data = preprocess_pdf(pdf_path)\n",
    "\n",
    "# if preprocessed_pdf_data:\n",
    "#     found_pages = find_line_in_json(preprocessed_pdf_data, target_line)\n",
    "#     if found_pages:\n",
    "#         print(f\"The target line found on pages: {found_pages}\")\n",
    "#     else:\n",
    "#         print(\"The target line was not found in the document.\")\n",
    "# else:\n",
    "#     print(\"No content found in the PDF.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
