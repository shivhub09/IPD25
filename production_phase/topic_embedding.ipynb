{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_normalize_text(text):\n",
    "    text = ''.join([char.lower() for char in text if char.isalnum() or char.isspace()])\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "def umass_coherence(doc_word_matrix, word_id_dict, topics):\n",
    "  print(\"umass coherence\")\n",
    "  \"\"\"\n",
    "  Calculates Umass coherence score for LDA model\n",
    "\n",
    "  Args:\n",
    "      doc_word_matrix: Document-word matrix (sparse matrix)\n",
    "      word_id_dict: Dictionary mapping word IDs to words\n",
    "      topics: LDA model topics\n",
    "\n",
    "  Returns:\n",
    "      Umass coherence score\n",
    "  \"\"\"\n",
    "  eps = 1e-12\n",
    "  coherence = 0.0\n",
    "  _, vocab_size = doc_word_matrix.shape\n",
    "\n",
    "  for m, x in enumerate(topics):\n",
    "    cluster_sum = np.sum(x)\n",
    "    for i, j, v in zip(doc_word_matrix.indptr[:-1], doc_word_matrix.indices, doc_word_matrix.data):\n",
    "      if x[j] > 0:  # Check if word is present in the topic\n",
    "        coherence += v / (cluster_sum + eps)\n",
    "  coherence /= (vocab_size * (vocab_size - 1))\n",
    "  return coherence\n",
    "\n",
    "\n",
    "def customize_tfidf_vectorizer(documents):\n",
    "    print(\"Cleaning documents...\")\n",
    "    cleaned_documents = [clean_and_normalize_text(doc) for doc in documents]\n",
    "\n",
    "    tokenized_documents = [word_tokenize(doc) for doc in cleaned_documents]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=2000,\n",
    "        stop_words=list(ENGLISH_STOP_WORDS),  \n",
    "        ngram_range=(1, 2),  \n",
    "        min_df=2,  \n",
    "        max_df=0.95,  \n",
    "        sublinear_tf=True  \n",
    "    )\n",
    "\n",
    "    document_matrix = vectorizer.fit_transform(cleaned_documents)\n",
    "    return document_matrix, vectorizer, tokenized_documents  # Return tokenized_documents\n",
    "\n",
    "\n",
    "\n",
    "def check_and_merge_topics(topics, threshold=0.9):\n",
    "  print(\"Checking and removing duplicate topics\")\n",
    "  \"\"\"\n",
    "  Checks for highly similar topics and merges them\n",
    "\n",
    "  Args:\n",
    "      topics: List of LDA topics (each topic is a word distribution)\n",
    "      threshold: Similarity threshold (0 to 1) for merging\n",
    "\n",
    "  Returns:\n",
    "      List of merged topics\n",
    "  \"\"\"\n",
    "  merged_topics = []\n",
    "  seen_topics = set()\n",
    "  for topic in topics:\n",
    "    most_similar_topic = None\n",
    "    max_similarity = 0\n",
    "    for seen_topic in seen_topics:\n",
    "      similarity = np.dot(topic, seen_topic) / (np.linalg.norm(topic) * np.linalg.norm(seen_topic))\n",
    "      if similarity > max_similarity:\n",
    "        max_similarity = similarity\n",
    "        most_similar_topic = seen_topic\n",
    "    if max_similarity < threshold:\n",
    "      merged_topics.append(topic)\n",
    "      seen_topics.add(tuple(topic)) \n",
    "    else:\n",
    "      pass\n",
    "  return merged_topics\n",
    "\n",
    "def find_optimal_num_topics(documents, max_topics=25):\n",
    "    print(\"Finding the optimal number of topics\")\n",
    "    document_matrix, vectorizer, tokenized_documents = customize_tfidf_vectorizer(documents)\n",
    "\n",
    "    coherence_values = []\n",
    "    for num_topics in range(2, max_topics + 1):\n",
    "        lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
    "        lda.fit(document_matrix)\n",
    "        topics = lda.components_\n",
    "        coherence_values.append(umass_coherence(document_matrix, vectorizer.get_feature_names_out(), topics))\n",
    "\n",
    "    optimal_num_topics = np.argmax(coherence_values) + 2  # Adding 2 because the loop starts from 2\n",
    "    return optimal_num_topics\n",
    "\n",
    "def refine_similarity_with_word_embeddings(merged_topics, word2vec_model):\n",
    "    print(\"Refining similarity using word embeddings\")\n",
    "    refined_topics = []\n",
    "    for topic in merged_topics:\n",
    "        if isinstance(topic, np.ndarray):  # Check if topic is a numpy array (vector)\n",
    "            topic_embedding = topic\n",
    "        else:\n",
    "            topic_embedding = np.mean([word2vec_model.wv[word] for word in topic], axis=0)\n",
    "        refined_topics.append(topic_embedding)\n",
    "    return refined_topics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
