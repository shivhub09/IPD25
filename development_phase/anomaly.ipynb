{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No anomaly detected in segment 1: Topic modeling is an interesting field\n",
      "No anomaly detected in segment 2: Latent Dirichlet Allocation is used for topic modeling\n",
      "No anomaly detected in segment 3: Anomaly detection is crucial for identifying unusual patterns\n",
      "No anomaly detected in segment 4: LDA and anomaly detection can be combined for specific use cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shivam\\anaconda3\\Lib\\site-packages\\gensim\\models\\ldamodel.py:847: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Sample document\n",
    "document = \"\"\"\n",
    "Topic modeling is an interesting field. Latent Dirichlet Allocation is used for topic modeling.\n",
    "Anomaly detection is crucial for identifying unusual patterns. LDA and anomaly detection can be combined for specific use cases.\n",
    "\"\"\"\n",
    "\n",
    "# Split the document into segments (sentences) after each full stop\n",
    "segments = document.split('.')\n",
    "\n",
    "# Tokenize and preprocess each segment\n",
    "tokenized_segments = [segment.strip() for segment in segments if segment.strip()]\n",
    "\n",
    "# Create a dictionary representation of the document\n",
    "dictionary = corpora.Dictionary([tokenized_segments])\n",
    "\n",
    "# Convert the tokenized document into a document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(segment.split()) for segment in tokenized_segments]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = LdaModel(doc_term_matrix, num_topics=6, id2word=dictionary)\n",
    "\n",
    "# Function to get the topic distribution of a segment\n",
    "def get_topic_distribution(segment):\n",
    "    bow_vector = dictionary.doc2bow(segment.split())\n",
    "    topics = lda_model.get_document_topics(bow_vector)\n",
    "    topic_distribution = np.zeros(lda_model.num_topics)\n",
    "\n",
    "    for topic, prob in topics:\n",
    "        topic_distribution[topic] = prob\n",
    "\n",
    "    return topic_distribution\n",
    "\n",
    "# Get the overall topic distribution of the document\n",
    "overall_topic_distribution = np.mean([get_topic_distribution(segment) for segment in tokenized_segments], axis=0)\n",
    "\n",
    "# Function to detect anomalies in a segment\n",
    "def detect_anomaly(segment, threshold):\n",
    "    segment_topic_distribution = get_topic_distribution(segment)\n",
    "    similarity_score = cosine_similarity([segment_topic_distribution], [overall_topic_distribution])[0, 0]\n",
    "\n",
    "    if similarity_score < threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "threshold = 0.9\n",
    "\n",
    "for i, segment in enumerate(tokenized_segments):\n",
    "    if detect_anomaly(segment, threshold):\n",
    "        print(f\"Anomaly detected in segment {i + 1}: {segment}\")\n",
    "    else:\n",
    "        print(f\"No anomaly detected in segment {i + 1}: {segment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 is an anomaly.\n",
      "Document 1 is an anomaly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shivam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shivam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
